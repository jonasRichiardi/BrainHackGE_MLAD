---
title: "Basic example TADPOLE forecasting for leaderboard"
author: "Jonas Richiardi"
date: "3/17/2019"
output: html_document
---

# INTRO

This is a basic R forecasting script for the TADPOLE leaderboard data, initially used during the BrainHack Open Geneva <http://brainhack.ch> for the Dementia forecasting machine learning project.

It is freely inspired by `TADPOLE_SimpleForecastExampleLeaderboard.py` by Neil Oxtoby, itself derived from the matlab version by Daniel C. Alexander, Neil P. Oxtoby, and Razvan Valentin-Marinescu, available at the official TADPOLE challenge git repo <https://github.com/noxtoby/TADPOLE>.

Usage of the ADNI data present in the TADPOLE challenge, and processed here, is governed by the ADNI data usage and citation policy.

# PACKAGE LOADING AND DATA PREP

```{r setup_datprep, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# General programming constructs
library(plyr)
library(tidyverse)
library(glue)

# ML libraries
library(caret)
library(randomForest)

# General computation
library(doParallel)

# Viz
library(visdat)
library(directlabels)

# where the TADPOLE challlenge csv files are (D1_D2, LB1_LB2 and friends)
data_path<-'../../data'
results_path<-'../../results'

data_path.D1D2 = file.path(data_path, 'TADPOLE_D1_D2.csv')
data_path.LB1LB2 = file.path(data_path, 'TADPOLE_LB1_LB2.csv')

# read in files 
# this throws some parsing errors in particular with variables 
# AVEMTEMP_BAIPETNMRC_09_12_16
# TAU_UPENNBIOMK9_04_19_17 (Tau levels in pg/ml, which is sometimes coded here as above or below certain values as in "<80" rather than a double)
# PTAU_UPENNBIOMK9_04_19_17 (ditto with phospho-tau)
# but let's ignore for now
df<-readr::read_csv(data_path.D1D2)

# do some cleanup and compute useful derived quantities
# TODO: convert viscode to ordered factor
# TODO: Add current age (use EXAMDATE) as AGEnow
df<-dplyr::mutate(df,
                  DXnow=unlist(plyr::llply(stringr::str_split(DX,' ',simplify=FALSE), .fun=function(this_dx){tail(this_dx,1)})),   # DXnow is only current diagnosis
                  DXnow=factor(DXnow, levels=c('NL','MCI','Dementia')),
                  DX=factor(DX),                                   # DX can also encode conversions 
                  Ventricles_ICV=Ventricles/ICV_bl,                # Note - using baseline ICV due to less missingness
                  PTGENDER=factor(PTGENDER),
                  Exam_months_since_2K=lubridate::time_length(EXAMDATE-lubridate::ymd("2000,01,01"), "months"), # not really needed, just for compat with python example
                  AV45_bl=as.numeric(AV45_bl),
                  PIB_bl=as.numeric(PIB_bl)) 

# get train/test splits (LB1/LB2)
df_lb<-readr::read_csv(data_path.LB1LB2)

df<-merge(df, df_lb[,c('RID','VISCODE','EXAMDATE','LB1','LB2')], by=c('RID','VISCODE','EXAMDATE'),sort=FALSE) %>% 
  dplyr::select(RID, PTID, SITE, VISCODE, EXAMDATE, LB1, LB2, DXnow, ADAS13, Ventricles_ICV, dplyr::everything())

# for training, keep only data until 2010
# NOTE we could actually use everything before that date, not just LB1/LB2
df_tr<-dplyr::filter(df, LB1==1 || LB2==1, EXAMDATE<"2010-04-01")


N_tr=length(unique(dplyr::filter(df_tr,LB1==1)$RID))
N_tr_pred=length(unique(dplyr::filter(df_tr,LB2==1)$RID))

print(glue::glue('Got {N_tr} training subjects and {N_tr_pred} prediction subjects for leaderboard'))
print(table(df_tr$DXnow, useNA='ifany'))

# XXX RESUME PREP LINE 140 simpleForecastExampleLeaderboard.py

# check out a random subject
print(dplyr::filter(df_tr,RID %in% sample_n(df,1)$RID)[,c('RID','EXAMDATE','AGE','DXnow','ADAS13','Ventricles_ICV')])

# Explore missingness for a subset of 10 random subjects
df_rnd<-dplyr::filter(df_tr, RID %in% sample(unique(df$RID),10))[,c('RID','EXAMDATE','DXnow','ADAS13','Ventricles_ICV')]
visdat::vis_miss(df_rnd)

# "There is no excuse for not looking at data" - show ADAS and DX trajectories for same subjects
my_plot_settings<-list(geom_line(aes(group=RID, colour=DXnow), size=2),
                       geom_point(),
                       theme_bw(),
                       geom_dl(aes(label = RID), method = "first.points"))
p1<-ggplot(df_rnd, aes(EXAMDATE, ADAS13))+my_plot_settings
p2<-ggplot(df_rnd, aes(EXAMDATE, Ventricles_ICV))+my_plot_settings
gridExtra::grid.arrange(p1,p2,nrow=2)

```


# FORECASTS

Here we must provide monthly forecasts for seven years (84 months), starting on 20100501, ending on 20170401.

To do so, we can use any subject data prior to 

The quantities to predict are 
	- clinical status (NL/MCI/AD)
		- NL, MCI, AD
		- must provide relative probabilities of each 
		- evaluated by: multi-class AUC
	- ADAS-Cog13 score
		- must provide score + upper/lower 50% CI 
		- evaluated by MAE 
	- Normalized Ventricle Volume (Ventricles / ICV)
		- must provide NVV + upper/lower 50% CI 
		- evaluated by MAE
		
Some algorithms to explore for forecasts
  - Beta regression

## Vars prep

```{r model_prep}

vars_target<-c('DXnow','ADAS13','Ventricles_ICV')
vars_pred<-c('AGE')
vars_all<-c(vars_target, vars_pred)

# compute class-conditional imputation values for cases with missing data
# Note! Using these for LB1 crossval is not correct since this looks is trained on the whole LB1
# IDEA can easily improve this by mabye looking at mode of distribution or fitting a beta (for Ventricles_ICV or ADA)
df_impute <- df_tr %>%
  dplyr::group_by(DXnow) %>%
  dplyr::summarise(imp.Ventricles_ICV=median(Ventricles_ICV, na.rm=TRUE),
                   imp.Ventricles_ICV50_CILower=quantile(Ventricles_ICV,0.25, na.rm=TRUE),
                   imp.Ventricles_ICV50_CIUpper=quantile(Ventricles_ICV,0.75, na.rm=TRUE),
                   imp.ADAS13=median(ADAS13, na.rm=TRUE),
                   imp.ADAS1350_CILower=quantile(ADAS13,0.25, na.rm=TRUE),
                   imp.ADAS1350_CIUpper=quantile(ADAS13,0.75, na.rm=TRUE),
                   imp.CNRelativeProbability=0.33,
                   imp.MCIRelativeProbability=0.33,
                   imp.ADRelativeProbability=0.34)
# row 1 = NL - most stay normal
df_impute[1,c('imp.CNRelativeProbability','imp.MCIRelativeProbability','imp.ADRelativeProbability')]<-c(0.7, 0.2, 0.1)
# row 2 = MCI - slighty more than half convert
df_impute[2,c('imp.CNRelativeProbability','imp.MCIRelativeProbability','imp.ADRelativeProbability')]<-c(0.01, 0.39, 0.6)
# row 3 = dementia - low chance of reverting to CN
df_impute[3,c('imp.CNRelativeProbability','imp.MCIRelativeProbability','imp.ADRelativeProbability')]<-c(0.01, 0.05, 0.95)
# row 4 = N/A - keep flat prior
imp_rowidx_NA<-4

```

## Eval (CV on LB1)

This has 819 subjects in the pre-2010/04 data.

```{r model_eval}


# setup CV
n_folds<-10
n_repeats<-30

# Compute metrics: MAE + mAUC



```


## Test (train LB1+LB2->predict LB2->test LB4)

There are 110 LB2 subjects.

One simple way to do this is to get a prediction for the last available observation of a subject in LB2, and forecast this same value for all 84 months.

A simple improvement to the baseline DX-conditional imputation technique implemented here is to use carry-last-observation forward.

Another simple improvement is to use the DX-conditional imputation as a regularizer for the last observation.

Most performance gains are expected by 
  - Computing age at exam date rather than age at baseline, and adding this as a time-varying covariate
  - Adding imaging and other covariates 
  - Using different predictors for each of the three prediction targets
  - Including subject clustering (e.g. by random effects modelling)
  - Trajectory modelling (true time series rather than clustered points)

```{r model_test}
source('prediction_models_jonas.R')

doParallel::registerDoParallel(6)

########## Train on full LB1+LB2
# We could also remove last full observation for each subject in LB2 < 2010-04 to avoid overfitting
my_formula<-as.formula(paste('Ventricles_ICV',paste(setdiff(vars_all, 'Ventricles_ICV'), collapse=' + '), sep='~'))
mod.NVV<-randomForest::randomForest(my_formula, dplyr::filter(df_tr, complete.cases(df_tr[,vars_all])), ntree=501, importance=FALSE)
my_formula<-as.formula(paste('ADAS13',paste(setdiff(vars_all, 'ADAS13'), collapse=' + '), sep='~'))
mod.ADAS13<-randomForest::randomForest(my_formula, dplyr::filter(df_tr, complete.cases(df_tr[,vars_all])), ntree=501, importance=FALSE)
my_formula<-as.formula(paste('DXnow',paste(setdiff(vars_all, 'DXnow'), collapse=' + '), sep='~'))
mod.DXnow<-randomForest::randomForest(my_formula, dplyr::filter(df_tr, complete.cases(df_tr[,vars_all])), ntree=501, importance=FALSE)


########## Predict on LB2
n_months<-84

# empty frame for one subject
df_res.empty<-data.frame(RID=rep(NaN, n_months),
                               ForecastMonth=seq(1:n_months),
                               ForecastDate=strftime(seq(as.Date("2010-05-01"), as.Date("2017-04-01"), by="month"), format="%Y-%m"), # from 2010-05 to 2017-04
                               CNRelativeProbability=rep(NaN, n_months),
                               MCIRelativeProbability=rep(NaN, n_months),
                               ADRelativeProbability=rep(NaN, n_months),
                               ADAS13=rep(NaN, n_months),
                               ADAS1350_CILower=rep(NaN, n_months),
                               ADAS1350_CIUpper=rep(NaN, n_months),
                               Ventricles_ICV=rep(NaN, n_months),
                               Ventricles_ICV50_CILower=rep(NaN, n_months),
                               Ventricles_ICV50_CIUpper=rep(NaN, n_months)
                               )

# do subject-by-subject predictions for LB2 subjects
df_all_results<-plyr::ldply(unique(dplyr::filter(df_tr, LB2==1)$RID), .fun=function(this_RID){
  print(this_RID)
  # copy correctly structured empty dataframe
  df_res<-df_res.empty
  # tag subject
  df_res$RID=this_RID
  
  # find last available observation for imputation
  df_tmp<-dplyr::filter(df_tr, RID==this_RID)[,c(vars_all, 'EXAMDATE')] %>%
    arrange(EXAMDATE)
  # FIXME need to handle all-NA case
  last_non_empty<-sapply(df_tmp, function(x) max(which(!is.na(x))))
  # locate appropriate row of imputation data frame, to get DX-conditional imputation
  imp_rowidx_use<-match(df_tmp[last_non_empty['DXnow'],'DXnow'], df_impute$DXnow)
  
  # See if we can get a complete dataframe to use a predictive model
  # otherwise just impute
  df_tmp_cc<-dplyr::filter(df_tmp, complete.cases(df_tmp[,vars_all]))
  if (dim(df_tmp_cc)[1] > 0) {
    pred_NVV<-predict(mod.NVV, tail(df_tmp_cc,1))
    pred_ADAS13<-predict(mod.ADAS13, tail(df_tmp_cc,1))
    pred_DXnow<-predict(mod.DXnow, tail(df_tmp_cc,1), type="prob")
    pred_DXnow.CNRelativeProbability<-pred_DXnow[,'NL']
    pred_DXnow.MCIRelativeProbability<-pred_DXnow[,'MCI']
    pred_DXnow.ADRelativeProbability<-pred_DXnow[,'Dementia']
    # TODO also compute intervals around predictions rather than imputing
  } else {
    pred_NVV<-df_impute[imp_rowidx_use,]$imp.Ventricles_ICV
    pred_ADAS13<-df_impute[imp_rowidx_use,]$imp.ADAS13
    pred_DXnow.CNRelativeProbability<df_impute[imp_rowidx_use,]$imp.CNRelativeProbability
    pred_DXnow.MCIRelativeProbability<-df_impute[imp_rowidx_use,]$imp.MCIRelativeProbability
    pred_DXnow.ADRelativeProbability<-df_impute[imp_rowidx_use,]$imp.ADRelativeProbability
  }
  
  # add predictions here - replace by your own outputs for those you want to predict
  df_res$CNRelativeProbability<-pred_DXnow.CNRelativeProbability
  df_res$MCIRelativeProbability<-pred_DXnow.MCIRelativeProbability
  df_res$ADRelativeProbability<-pred_DXnow.ADRelativeProbability
  df_res$ADAS13<-pred_ADAS13
  df_res$ADAS1350_CILower<-df_impute[imp_rowidx_use,]$imp.ADAS1350_CILower
  df_res$ADAS1350_CIUpper<-df_impute[imp_rowidx_use,]$imp.ADAS1350_CIUpper
  df_res$Ventricles_ICV<-pred_NVV
  df_res$Ventricles_ICV50_CILower<-df_impute[imp_rowidx_use,]$imp.Ventricles_ICV50_CILower
  df_res$Ventricles_ICV50_CIUpper<-df_impute[imp_rowidx_use,]$imp.Ventricles_ICV50_CIUpper
  
  # return filled dataframe
  return(df_res)
}, .parallel=TRUE, .progress="text")

```


# PREPARE SUBMISSION FILE 

```{r prepare_sub}

# submission name - no spaces, no special chars, no funny business
name_submission<-'RFDXnow_RFNVV_RFADAS13_PTGENDER_MidTempBl_conditionalImputation.pre201004'
output_file = file.path(results_path, glue::glue('TADPOLE_Submission_Leaderboard_{name_submission}.csv'))

# map from df variables to submission csv variables
map_out_format=c('RID'='RID',
  'Forecast Month' = 'ForecastMonth',
  'Forecast Date' = 'ForecastDate',
  'CN relative probability' = 'CNRelativeProbability',
  'MCI relative probability' = 'MCIRelativeProbability',
  'AD relative probability' = 'ADRelativeProbability',
  'ADAS13'= 'ADAS13',
  'ADAS13 50% CI lower' = 'ADAS1350_CILower',
  'ADAS13 50% CI upper' = 'ADAS1350_CIUpper',
  'Ventricles_ICV'= 'Ventricles_ICV',
  'Ventricles_ICV 50% CI lower' = 'Ventricles_ICV50_CILower',
  'Ventricles_ICV 50% CI upper' = 'Ventricles_ICV50_CIUpper')

df_all_results_sub<-df_all_results %>% 
  dplyr::rename(!!map_out_format)

readr::write_csv(df_all_results_sub, output_file)


```

